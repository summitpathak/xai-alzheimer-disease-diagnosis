# -*- coding: utf-8 -*-
"""thesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4VzXC8dlNAcc--gU07Np5ndbvYQilsd
"""

from google.colab import drive
drive.mount('/content/mydrive')

path='/content/mydrive/My Drive/Thesis/oasis_longitudinal.csv'
import pandas as pd

data= pd.read_csv(path)

data.head()

"""Converting Categorical Data to Numerical Data"""

data['M/F'] = [1 if each == "M" else 0 for each in data['M/F']]
data['Group'] = [1 if each == "Demented" or each == "Converted" else 0 for each in data['Group']]

data.head()

data.shape

counted = data.groupby(["EDUC"]).size()
counted

"""Correlation Between Attributes"""

# Select only numerical columns for correlation calculation
numerical_data = data.select_dtypes(include=['number'])

# Calculate the correlation matrix
correlation_matrix = numerical_data.corr()

# Proceed with sorting the correlation values
data_corr = correlation_matrix['EDUC'].sort_values(ascending=False)

# Display the results
data_corr

attributes = ["Group", "CDR", "M/F", "SES", "ASF"]

"""Checking For Missing/Null Values"""

data.isnull().sum()

"""Taking median values for the missing values of MMSE and SES"""

median = data['MMSE'].median()
data['MMSE'].fillna(median, inplace=True)
data.isnull().sum()
median = data['SES'].median()
data['SES'].fillna(median, inplace=True)
data.isnull().sum()

"""Prepare the data for X and y where, X = The columns/features for making the prediction y = The predicted value"""

y = data['Group'].values
X = data[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']]

"""Train-Test Split"""

from sklearn.model_selection import train_test_split
# taking 80% as train and 20% as test
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size= 0.20, random_state=42)

df_ytrain = pd.DataFrame(y_trainval)
df_ytest = pd.DataFrame(y_test)

print('In Training Split:')
print(df_ytrain[0].value_counts())

print('\nIn Testing Split:')
print(df_ytest[0].value_counts())

"""Scaling the data"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
# here StandardScaler() means z = (x - u) / s
scaler = StandardScaler().fit(X_trainval)
#scaler = MinMaxScaler().fit(X_trainval)
X_trainval_scaled = scaler.transform(X_trainval)
X_test_scaled = scaler.transform(X_test)

X_trainval.describe()

!pip install shap
!pip install lime
import numpy as np
import sklearn
import shap
import lime
from lime import lime_tabular
import time

"""SVM Linear Kernel"""

from sklearn.svm import SVC # will make a SVM for classification
from sklearn.metrics import confusion_matrix,precision_score, accuracy_score, recall_score, roc_curve, auc, ConfusionMatrixDisplay #import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

shap.initjs()
svc_linear = sklearn.svm.SVC(kernel='linear', probability=True)
svc_linear.fit(X_trainval_scaled, y_trainval)

cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()


train_score = svc_linear.score(X_trainval_scaled, y_trainval)
test_score = svc_linear.score(X_test_scaled, y_test)
y_predict = svc_linear.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# Shap Explanation
explainer = shap.KernelExplainer(svc_linear.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap.force_plot(explainer.expected_value[0], shap_values[0], X_test_scaled)

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[59, 0],
                 [16, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented','Demented'],mode='classification')
exp = explainer.explain_instance(data_row=X_test_scaled[31],
    predict_fn=svc_linear.predict_proba)
exp.show_in_notebook(show_table=True)

"""SVM radial basis function kernel"""

!pip install shap
import shap
import sklearn
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # Importing confusion_matrix and ConfusionMatrixDisplay
import matplotlib.pyplot as plt

shap.initjs()
svc_rbf = sklearn.svm.SVC(kernel='rbf', probability=True)
svc_rbf.fit(X_trainval_scaled, y_trainval)

cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()


train_score = svc_rbf.score(X_trainval_scaled, y_trainval)
test_score = svc_rbf.score(X_test_scaled, y_test)
y_predict = svc_rbf.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(svc_rbf.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)

# Select SHAP values for the first class (index 0)
shap_values_class0 = shap_values[:, :, 0]

# Use shap_values_class0 for the force_plot, but for a single instance
# Here, we're plotting the explanation for the first instance in the test set (index 0)
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[58, 0],
                 [17, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=svc_rbf.predict_proba)
exp.show_in_notebook(show_table=True)

"""KNN"""

shap.initjs()
knn = sklearn.neighbors.KNeighborsClassifier()
knn.fit(X_trainval_scaled, y_trainval)
cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()
train_score = knn.score(X_trainval_scaled, y_trainval)
test_score = knn.score(X_test_scaled, y_test)
y_predict = knn.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(knn.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap_values_class0 = shap_values[:, :, 0]
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[53, 0],
                 [22, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=knn.predict_proba)
exp.show_in_notebook(show_table=True)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

shap.initjs()
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_trainval_scaled, y_trainval)

cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()

train_score = rfc.score(X_trainval_scaled, y_trainval)
test_score = rfc.score(X_test_scaled, y_test)
y_predict = rfc.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(rfc.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap_values_class0 = shap_values[:, :, 0]
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])



#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=rfc.predict_proba)
exp.show_in_notebook(show_table=True)

"""Logistic Regression"""

shap.initjs()
linear_lr = sklearn.linear_model.LogisticRegression()
linear_lr.fit(X_trainval_scaled, y_trainval)


cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()

train_score = linear_lr.score(X_trainval_scaled, y_trainval)
test_score = linear_lr.score(X_test_scaled, y_test)
y_predict = linear_lr.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(linear_lr.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap_values_class0 = shap_values[:, :, 0]
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[56, 0],
                 [19, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=linear_lr.predict_proba)
exp.show_in_notebook(show_table=True)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

"""Decision Tree"""

shap.initjs()
import sklearn.tree
dtree = sklearn.tree.DecisionTreeClassifier(min_samples_split=2)
dtree.fit(X_trainval_scaled, y_trainval)


cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()

train_score = dtree.score(X_trainval_scaled, y_trainval)
test_score = dtree.score(X_test_scaled, y_test)
y_predict = dtree.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(dtree.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap_values_class0 = shap_values[:, :, 0]
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[59, 0],
                 [16, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=dtree.predict_proba)
exp.show_in_notebook(show_table=True)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

"""Neural Network"""

shap.initjs()
from sklearn.neural_network import MLPClassifier
nn = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(5, 2), random_state=0)
nn.fit(X_trainval_scaled, y_trainval)


cm = confusion_matrix(y_test, svc_linear.predict(X_test_scaled))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nondemented', 'Demented']) # Now ConfusionMatrixDisplay is recognized
disp.plot(values_format='d')
plt.show()

train_score = nn.score(X_trainval_scaled, y_trainval)
test_score = nn.score(X_test_scaled, y_test)
y_predict = nn.predict(X_test_scaled)
test_recall = recall_score(y_test, y_predict)
fpr, tpr, thresholds = roc_curve(y_test, y_predict)
test_auc = auc(fpr, tpr)
print("Train accuracy ", train_score)
print("Test accuracy ", test_score)
print("Test recall", test_recall)
print("Test AUC", test_auc)

# explain all the predictions in the test set
explainer = shap.KernelExplainer(nn.predict_proba, X_test_scaled)
shap_values = explainer.shap_values(X_test_scaled)
shap_values_class0 = shap_values[:, :, 0]
shap.force_plot(explainer.expected_value[0], shap_values_class0[0], X_test_scaled[0], feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

import numpy as np
from mlxtend.evaluate import mcnemar_table
tb = mcnemar_table(y_target=y_test,
                   y_model1=y_test,
                   y_model2=y_predict)

print(tb)

tb_b = np.array([[60, 0],
                 [15, 0]])
from mlxtend.evaluate import mcnemar

chi2, p = mcnemar(ary=tb_b, corrected=True)
print('chi-squared:', chi2)
print('p-value:', p)

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]  # Select SHAP values for class 1

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'])

#summary of SHAP
# shap_values[1] contains the SHAP values for the second class (index 1)
# Selecting all instances (:) and all features (:)
shap_values_class1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class1,
                  X_test_scaled,
                  feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
                  plot_type="bar")

#Lime Explanation
shap.initjs()
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_trainval_scaled),
    feature_names=['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],
    class_names=['Nondemented', 'Demented'],
    mode='classification')
exp = explainer.explain_instance(
    data_row=X_test_scaled[31],
    predict_fn=nn.predict_proba)
exp.show_in_notebook(show_table=True)

#Import libraries that will allow you to use keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU
from keras import metrics
!pip install keras-metrics #It doesn't come with Google Colab
import keras_metrics as km #when compiling
import keras
import numpy as np
from numpy import array

